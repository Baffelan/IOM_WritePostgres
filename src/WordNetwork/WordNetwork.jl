
"""
WordNetwork is a struct for handling operations for which we need a mixture of information.

This includes:
    TextGraph structure (text_graph)
    SVD embedding (embedding)
    Dictionary from word to index in the text_graph and embedding (token_idx)
    
    Alignment transformation (aligning_graph)

"""
mutable struct WordNetwork
    # raw::Dict{String, Any}
    text_graph::AbstractArray{Bool}
    embedding::NamedTuple{(:L̂, :R̂), Tuple{Matrix{Float32}, Matrix{Float32}}}
    token_idx::Dict{String, Int}

    # Optional
    aligning_matrix::Union{Matrix{Float32}, Nothing}
end

Base.show(io::IO, ::MIME"text/plain", f::WordNetwork) = @printf(io, "WordNetwork with %s unique tokens",length(keys(f.token_idx)))


# function WordNetwork(raw::Dict, emb_d::Int)
# """
#   this function is a holdover from when Connor was making networks for individual articles. 
#   For these articles, the edges are generated by sequential words not coocurrence in sentence.
# """
#     text_graph = lemma_graph.(raw["body"])
#     embedding = DotProductGraphs.svd_embedding(Matrix(adjacency_matrix(text_graph)), emb_d)

#     tokens = get_prop.([text_graph], 1:nv(text_graph),:token)
#     flip_enumerate(l) = map(x-> (l[x], x), eachindex(l))
#     etokens = flip_enumerate(tokens)
#     token_idx = Dict(etokens)

#     alignment_matrix = nothing    

#     WordNetwork(raw, text_graph, embedding, token_idx, alignment_matrix)
# end
"""
Generates and returns both the coocurrence matrix for a piece of formatted text (ideally all lowercase, no numbers, and the only punctuation is a fullstop), and the  token=>token_idx dictionary for that coocurence matrix.

# Arguments
- 'text::String': The string (usually preprocessed in some way) that will be used to create a coocurrence matrix.
- 'filtered_word_counts::Dict{T, Int} where {T<:AbstractString}': A dictionary from token=>token_idx wich contains tokens that will be used in the network (tokens that have been used frequently enough).
- 'unit::AbstractChar': The character which splits the 'text' to define nodes.
- 'coocur::AbstractChar': The character which splits the 'text' to define a relation.
"""
function coocurrence_matrix(text::String, filtered_word_counts::Dict{T, Int}; unit::AbstractChar=' ', coocur::AbstractChar='.')where{T<:AbstractString}
    words = rsplit(replace(text, coocur=>" "), unit)
    unique!(words)
    words = words[words.!=""]
    println("num words before filter: ", length(words))
    words = words[in.(words, [keys(filtered_word_counts)])]
    println("num words after filter: ", length(words))


    token_idx = Dict(zip(words, 1:length(words)))

    mat = zeros(Float32, (length(words), length(words)))

    
    sentences = rsplit(text, coocur)
    unique!(sentences)
    sentences = sentences[sentences.!=""]
    for sent in sentences
        
        for word1 in unique(rsplit(sent, unit)) 
            if in(word1, words)
                for word2 in unique(rsplit(sent, unit))
                    if in(word2, words)
                        @inbounds mat[token_idx[word1],token_idx[word2]]=1
                    end
                end
            end
        end
    end
    return mat, token_idx
end

function coocurrence_matrix_2(text::String, filtered_word_counts::Dict{T, Int}; unit::AbstractChar=' ', coocur::AbstractChar='.')where{T<:AbstractString}
    words = rsplit(replace(text, coocur=>" "), unit)
    unique!(words)
    words = words[words.!=""]
    println("num words before filter: ", length(words))
    words = words[in.(words, [keys(filtered_word_counts)])]

    token_idx = Dict(zip(words, 1:length(words)))
    # mat = zeros(Float32, (length(words), length(words)))

    
    sentences = rsplit(text, coocur)
    unique!(sentences)
    sentences = sentences[sentences.!=""]
    word_sent_mat = Matrix{Float32}(undef, (0,0))
    
    word_sent_mat = Float32.(hcat([in.(words, [rsplit(sent, unit)]) for sent in sentences]...))

    println(string("The coocurence matrix has ", length(sentences), " sentences and ", length(words)," words"))
    #@time mat = word_sent_mat*word_sent_mat'.>0

    # for sent in sentences
        
    #     for word1 in unique(rsplit(sent, unit)) 
    #         for word2 in unique(rsplit(sent, unit))
    #             if (word1 != "" && word2!="") 
    #                 @inbounds mat[token_idx[word1],token_idx[word2]]=1
    #             end
    #         end
    #     end
    # end
    return word_sent_mat, token_idx
end


"""
Creates a 'WordNetwork'.

# Arguments
- 'text::String': The string (usually preprocessed in some way) that will be used to create a coocurrence matrix.
- 'emb_dim::Int': The dimension required for the embedding.
- 'filtered_word_counts::Dict{T, Int} where {T<:AbstractString}': A dictionary from token=>token_idx wich contains tokens that will be used in the network (tokens that have been used frequently enough).
"""
function WordNetwork(text::String, emb_dim::Int, filtered_word_counts::Dict{T, Int}) where{T<:AbstractString}
    if length(text)>0
        text_graph, token_idx = coocurrence_matrix(text, filtered_word_counts)
        embedding=NamedTuple()

        embedding = DotProductGraphs.svd_embedding(text_graph, min(size(text_graph)[1],emb_dim), fast_svd)

        alignment_matrix = nothing    

        return WordNetwork(text_graph, embedding, token_idx, alignment_matrix)
    else
        empty_embedding = (L̂=Matrix{Float32}(undef, (0,0)), R̂=Matrix{Float32}(undef, (0,0)))
        return WordNetwork(Matrix{Float32}(undef, (0,0)), empty_embedding, Dict{String, Int64}(), nothing)
    end
end


# """
#     Calculates the direction vectors from 'E1' to 'E2', using 'dt1' and 'dt2' to find the overlap between the two matricess.
#     This is done for every word that the two WordNetworks have in common.
#     This will be done for matrices that have been alligned, if an aligning matrix exists.

# # Arguments
# - 'dt1::Dict{String, Int}': token=>token_idx dictionary for 'E1'.
# - 'E1::Matrix{Float32}': First embedding.
# - 'dt2::Dict{String, Int}': token=>token_idx dictionary for 'E2'.  
# - 'E2::Matrix{Float32}': Second embedding.
# """
# function word_embedding_dists(dt1::Dict{String, Int}, E1::Matrix{Float32}, dt2::Dict{String, Int}, E2::Matrix{Float32})
#     ints = intersect(keys(dt1), keys(dt2))
#     dists=[]
#     for int in ints
#         i1=dt1[int]
#         i2=dt2[int]
#         push!(dists, E2[i2,:].-E1[i1,:])
#     end
#     return Dict(zip(ints, dists))
# end

# """
#     Calculates the direction vectors from WN1, to WN2.
#     This is done for every word that the two WordNetworks have in common.
#     This will be done for matrices that have been alligned, if an aligning matrix exists.


# """
# function word_embedding_dists(wn1::WordNetwork, wn2::WordNetwork)
#     if (isnothing(wn1.aligning_matrix) || isnothing(wn2.aligning_matrix))
#         return word_embedding_dists(wn1.token_idx, wn1.embedding[:L̂], wn2.token_idx, wn2.embedding[:L̂])
#     else
#         E1 = wn1.embedding[:L̂]*wn1.aligning_matrix
#         E2 = wn2.embedding[:L̂]*wn2.aligning_matrix
#         return word_embedding_dists(wn1.token_idx, E1, wn2.token_idx, E2)
#     end
    
# end


sub_index(l::Dict, is) = map(x->l[x], is)
sub_index(l::AbstractArray, is) = map(x->l[x,:], is)

"""
    A function that returns the submatrix associated with the tokens in the 'tokens' Tuple
    This matrix is used to align the larger embedding matrix.

# Arguments
- 'wn::WordNetwork': WordNetwork.
- 'tokens::Base.AbstractVecOrTuple': Tokens to find the subembedding of.
- 'which_mat::Symbol': Which matrix to extract the subembedding of (:L̂, :R̂).
- 'aligned::Bool': Whether the returned subembedding should be aligned or not.
"""
function subembedding_from_tokens(wn::WordNetwork, tokens::Base.AbstractVecOrTuple; which_mat::Symbol=:L̂, aligned::Bool=false)
    alignment_locs = sub_index(wn.token_idx, tokens)
    if (aligned && !(isnothing(wn.aligning_matrix)))
        return hcat(sub_index(wn.embedding[which_mat]*wn.aligning_matrix, alignment_locs)...)'
    else
        return hcat(sub_index(wn.embedding[which_mat], alignment_locs)...)'
    end
    
end

"""
    A function that returns the submatrix associated with the tokens in the 'tokens' Tuple; this function allows for the possibility that not all 'tokens' are present in 'wn'. Any tokens that are not present will be ignored.
    This matrix is used to align the larger embedding matrix.

# Arguments
- 'wn::WordNetwork': WordNetwork.
- 'tokens::Base.AbstractVecOrTuple': Tokens to find the subembedding of.
- 'which_mat::Symbol': Which matrix to extract the subembedding of (:L̂, :R̂).
- 'aligned::Bool': Whether the returned subembedding should be aligned or not.
"""
function soft_subembedding_from_tokens(wn::WordNetwork; tokens::Base.AbstractVecOrTuple=ALIGNMENT, which_mat::Symbol=:L̂, aligned::Bool=false)

    token_intersect = tokens[[in(t, keys(wn.token_idx)) for t in tokens]]
    subembedding_from_tokens(wn, token_intersect; which_mat=which_mat, aligned=aligned)
end


"""
    Sets the aligning_matrix of wn to one which will transformation the matrix returned by "subembedding_from_tokens" to A.
    (or as close as possible)

# Arguments
- 'wn::WordNetwork': WordNetwork.
- 'A::AbstractArray': The array to align the 'wn' embedding to.
- 'tokens::Base.AbstractVecOrTuple': The tokens to use for the alignment.
"""
function aligning_matrix!(wn::WordNetwork; A::AbstractArray=REFMATRIX, tokens::Base.AbstractVecOrTuple=ALIGNMENT)
    subemb = soft_subembedding_from_tokens(wn, tokens=tokens)
    kept_tokens = [in(t, keys(wn.token_idx)) for t in tokens]
    if sum(kept_tokens)<(1+size(A)[2])
        wn.aligning_matrix=nothing
    else
        sub_A = A[kept_tokens,:]
        wn.aligning_matrix = subemb\sub_A

    end
end

Base.getindex(wn::WordNetwork, idxs::AbstractVecOrMat{String}) = subembedding_from_tokens(wn, idxs)
Base.getindex(wn::WordNetwork, idxs::String) = Base.getindex(wn, [idxs])