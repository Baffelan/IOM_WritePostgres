using DataFrames
include("word_count.jl")
include("format_text.jl")
include("../WordNetwork/articles_to_word_network.jl")

function text_and_date(data)
    all_text = join(data[!, "body"], " ")
    ftext = format_text(all_text)
    if nrow(data)>0
        return data[1,"date"], ftext
    else
        return nothing, ftext
    end
end


"""
    Extracts the fields of wf_s and matches them with the associated date, formated_text, and anomalies.
    This data is formated into a data frame that can be inserted into a postgreSQL schema that is generated by:

        CREATE TABLE ProcessedArticles(date DATE PRIMARY KEY, 
                                        day_text TEXT, 
                                        embedding DOUBLE PRECISION[][], 
                                        toxen_idx JSON, 
                                        anomalous_words TEXT[], 
                                        unseen_words TEXT[],
                                        anomalou_articles TEXT[]
                                        )
"""
function format_processed_to_df(wn_s::Base.AbstractVecOrTuple{WordNetwork}, raw_data::Base.AbstractVecOrTuple{DataFrame})
    date_text = text_and_date.(raw_data)
    date = [dt[1] for dt in date_text]
    day_text = [dt[2] for dt in date_text]

    embedding = [wn.embedding[:LÌ‚] for wn in wn_s]
    token_idx = [wn.token_idx for wn in wn_s]
    aligning_matrix = [wn.aligning_matrix for wn in wn_s]

    df = DataFrame(:date=>date, 
                   :day_text=>day_text, 
                   :embedding=>embedding,
                   :token_idx=>token_idx,
                   :aligning_matrix=>aligning_matrix)
    return df
end


fill_blank_dates!(df, dates) = df.date=dates

"""
Function to create the processed dataframe
"""
function create_processed_df(dfs_trace::Vector{DataFrame}, kw::String, alignment_tokens::Base.AbstractVecOrTuple{String}, refmatrix::Union{Nothing, AbstractArray}, emb_dim::Int, burnin::UnitRange, bwns::Vector{WordNetwork})

    word_nets = articles_to_word_network.(dfs_trace, [alignment_tokens], emb_dim, [refmatrix]);
    all_nets = vcat(bwns..., word_nets...)
    anom = find_anomalies_day(all_nets, burnin)
    

    net_df = format_processed_to_df(word_nets, dfs_trace)

    net_df[!, :anomalous_day] = [in(i,anom) for i in 1:length(word_nets)]
    net_df[!,:word_count] = word_count.(net_df[!,:day_text])
    net_df[!,:word_change] = vcat(Dict(), find_mean_word_dist(word_nets)...)
    net_df[!,:sentiment] = [mean(df.sentiment) for df in dfs_trace]
    # net_df[!,:most_shared] = most_shared.(dfs_trace)
    net_df[!,:articles] = [unique(df.uri) for df in dfs_trace]
    net_df[!,:keyword] .= kw
    return net_df


end
